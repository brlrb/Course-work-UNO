---
title: "STAT 4410/8416 Homework 3"
author: "Maharjan Bikram"
date: "Due on Oct 20, 2019"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
library(knitr)
opts_chunk$set(fig.align='center', message=FALSE, cache=TRUE)
output <- opts_knit$get("rmarkdown.pandoc.to")
if(!is.null(output)) {
  if(output=="html") opts_chunk$set(out.width = '400px') else
    opts_chunk$set(out.width='.6\\linewidth')
}
```

**1.** **Visualizing Relationships in Data:** Use the `MLB_teams` dataset in the `mdsr` package to create an informative data graphic that illustrates the relationship between winning percentage and payroll in context.

```{r}
library(mdsr)
library("data.table")

mlbTeams <- data.table(MLB_teams)

head(mlbTeams,1)

ggplot(mlbTeams, aes(WPct, payroll))+geom_point()+
   ggtitle("Data Graphic: MLB Winning % vs Payroll")+
   xlab("Winning Percentage")+
   ylab("Payroll")+
   scale_y_continuous(labels =scales::comma)

```


**2.** **Text Data analysis:** Download "lincoln-last-speech.txt" from Canvas which contains Lincoln's last public address. Now answer the following questions and include your codes.  

```{r}
library(tm)
library(data.table)
library(ggplot2)

```


   a) Read the text and store the text in `lAddress`. Show the first 70 characters from the first element of the text.  
   
```{r}
lAddress <- readLines('lincoln-last-speech.txt')

head(strsplit(lAddress, '')[[1]], 70)



```

   b) Now we are interested in the words used in his speech. Extract all the words from `lAddress`, convert all of them to lower case and store the result in `vWord`. Display first few words. 
   
```{r}

vWord <- tolower(lAddress)

head(vWord,1)

vWord <- removePunctuation(vWord)


head(strsplit(vWord, ' ')[[1]], 5)

```
   
   
   c) The words like `am`, `is`, `my` or `through` are not much of our interest and these types of words are called stop-words. The package `tm` has a function called `stopwords()`. Get all the English stop words and store them in `sWord`. Display few stop words in your report.  

```{r}

sWord <- stopwords(kind = "en")

head(sWord)



```  
   
   
   d) Remove all the `sWord` from `vWord` and store the result in `cleanWord`. Display first few clean words.  
   
```{r}

vWord <- unlist(strsplit(vWord," "))

cleanWord <- vWord[which(!vWord %in% sWord)]

head(cleanWord)

```

   
   e) `cleanWord` contains all the cleaned words used in Lincoln's address. We would like to see which words are more frequently used. Find 15 most frequently used clean words and store the result in `fWord`. Display first 5 words from `fWord` along with their frequencies.  
   
```{r}

fWord <- sort(table(cleanWord), decreasing = TRUE)[1:15]

head(fWord, 5)

```
   
   
   
   f) \label{coord} Construct a bar chart showing the count of each words for the 15 most frequently used words. Add a layer `+coord_flip()` with your plot.  
   
```{r}

fWord_freq <- data.table(fWord)


ggplot(fWord_freq)+
   geom_bar(aes(cleanWord,N), stat = "identity")+
   coord_flip()

```
   
   g) What is the reason for adding a layer `+coord_flip()` with the plot in question (2f). Explain what would happen if we would not have done that. 

  - The reason for adding a layer `+coord_flip()` is to flip the coordinates so that the axis are readable in the `cleanWord`. When `cleanWord` is in bottom x-axis, it is hard to read but when we flip the axis, it is much easier to read.
   
   
   h) The plot in question (2f) uses bar plot to display the data. Can you think of another plot that delivers the same information but looks much simpler? Demonstrate your answer by generating such a plot.  
   

 - The simpler chart for the "count" purpose would be using `geom_point()`

```{r}

ggplot(fWord_freq)+geom_point(aes(cleanWord,N), stat = "identity")+coord_flip()


```

   
**3.** **Answering Questions from Data:** Install package `nycflights13`. The package provides a data frame called `flights`. Answer the following questions using this data.  

```{r}

# dependencies
library(nycflights13)
library(lubridate)
library(dplyr)


```

   a) What month had the highest proportion of cancelled flights? What month had the lowest? Interpret any seasonal patterns. Please produce a plot that illustrates the proportion of cancelled flights for each month.  
   
```{r}

flight <- flights

flightsDT <- data.table(flights)

head(flightsDT, 1)

flight_cancel_month <- flightsDT %>%
   select(month, arr_delay) %>%
   group_by(month) %>%
   summarise(cancellationTotal = sum(is.na(arr_delay)),  totalFlights = length(arr_delay)) 


ggplot(flight_cancel_month)+
   geom_bar(aes(x = factor(month, labels = c("Jan","Feb","Mar","Apr", "May","Jun","Jul","Aug","Sep","Oct","Nov","Dec")),
                y = cancellationTotal/totalFlights), stat = "identity")+
   xlab("Week Number")+ 
   ylab("Cancelled Flights %")

```

 - The month Febuary had the highest number of calcellation and the month October had the lowest cancellation. October and November seems to have had about the same porportion of cancellation.

 - The reason why Febuary could have had the highest numbers of cancellation is because the flights are taken off from NYC and New Jersey which is on the East Coast. The weather are pretty much bad around that time with snow and the temperature droping below zero which makes flight taking off at risk. 

 - The reason October and November have had the lowest number of cancellation is because the weather are not yet getting worse and good for flights. It could also be that those months could have a lots of flights compared to other months and even though there are cancellations, the ratio is not that high.

   
   b) What plane (specified by the tailnum variable) traveled the most times from New York City airports in 2013? Plot the number of trips per week over the year.  
```{r}


flightsDT <- data.table(flights)

head(flightsDT,1)

flight_tailnum <- flightsDT[origin == "LGA" | origin == "JFK" & year == 2013, list(tailnum) ]


flight_tailnum <- flight_tailnum %>%
   select(tailnum) %>%
   filter(!is.na(tailnum)) %>%
   group_by(tailnum) %>%
   summarise(number_of_flights = length(tailnum)) %>%
   arrange(desc(number_of_flights))


flight_tailnum <- head(flight_tailnum, 1)


flight_tailnum_value <- c(flight_tailnum[1,1])

head(flight_tailnum_value,1)


flightsDT %>%
   filter(tailnum == flight_tailnum_value) %>%
   mutate(date = ymd(sprintf('%04d%02d%02d', year, month, day))) %>%
   mutate(week_number = week(date)) %>%
   group_by(week_number) %>%
   summarize(flight_number = n()) %>%
   ggplot()+
   geom_line(aes(x = week_number, y = flight_number))+
   xlab("Week Number")+ 
   ylab("Number of Flights from New York")


```  

   c) Use the flights and planes tables to answer the following questions: What is the oldest plane (specified by the tailnum variable) that flew from New York City airports in 2013? How many airplanes that flew from New York City are included in the planes table?  
   
```{r}

planesDT <- data.table(planes)

head(planesDT,1)

head(flightsDT,1)

head(flight_tailnum,1)
head(planesDT,1)


oldest_place_tailnum <- planesDT %>%
   rename(built = year) %>%
   right_join(flightsDT, by = "tailnum") %>%
   arrange(built) %>%
   select(tailnum, built) %>%
   head(1)

head(oldest_place_tailnum,1)

flight_count <- flight_tailnum %>%
   right_join(planesDT, by = "tailnum") %>%
   summarize(flight_count = n()) %>%
   select(flight_count) %>%
   head(1)


flight_count

```

  - The airplanes that flew from New York City that are included in the planes table is `3322`

   
   
   d) Use the flights and planes tables to answer the following questions: How many planes have a missing date of manufacture? What are the five most common manufacturers (Note: pay close attention to the same manufacturer being represented multiple times)? Has the distribution of manufacturer changed over time as reflected by the airplanes flying from NYC in 2013? Produce a plot that backs up your claim. (Hint: you may need to recode the manufacturer name and collapse rare vendors into a category called Other.)  
   
```{r}

mising_manufacture_date <- flightsDT%>%
   group_by(tailnum) %>%
   slice(1L) %>%
   inner_join(planesDT, by = "tailnum")%>%
   ungroup() %>%
   summarize(missing_date = sum(is.na(year.y)))

mising_manufacture_date
```   
 - The number of plances that have missing manufacture date is `70`
 
 
 
```{r}


head(planesDT,1)


manufactureDT <- 
   planesDT %>%
      mutate(original_manufacture = substr(manufacturer,1,6)) %>%
      mutate(year_copy = factor(cut(year, breaks=seq(from=1960, to=2015, by=5))))



five_common_manufacture  <- 
   manufactureDT %>%
      group_by(original_manufacture) %>%
      summarise(total_manufactured = length(original_manufacture)) %>%
      arrange(desc(total_manufactured)) %>%
      head(5)


five_common_manufacture
 
```
 - The five common manufacture is given above.
 
 
```{r}
 
flightsDT %>%
   left_join(manufactureDT, by = "tailnum") %>%
   select(year_copy, original_manufacture)%>%
   filter((!is.na(original_manufacture))) %>%
   mutate(count = 1) %>%
   ggplot(aes(x = year_copy, y = count, fill = original_manufacture)) +
   geom_bar(stat = "identity", position = position_fill()) +
   xlab("Year Range")+ 
   ylab("Percent")+
   theme(axis.text.x = element_text(angle = 60, hjust = 1))
 
 
```
   
 - The distribution has changed by looking at the chart above. During 1960, the majority of distribution is done by one single manufacturer but it looks like after around 1985, it started to get divided between several other manufacturer.
 
 
    
   e) Use the weather table to answer the following questions specifically for July, 2013: What is the distribution of temperature in terms of windspeed? What is the relationship between dewp and humid? What is the relationship between precip and visib? Please provide plots for each question.

```{r}

weatherDT <- data.table(weather)

head(weatherDT,1)

weatherDT %>%
   filter(month == 7) %>%
   filter(!is.na(wind_speed)) %>%
   ggplot(aes(x = wind_speed, y = temp)) +
   geom_point() +
   labs(x = "Wind Speed", y = "Temperature")

```
 - One observation I can conclude from the above graph is that higher the wind speed, the temperature can fall but it look pretty even to me.


```{r}

weatherDT %>%
   filter(month == 7) %>%
   filter(!is.na(wind_speed)) %>%
   ggplot(aes(x = dewp, y = humid)) +
   geom_point() +
   labs(x = "Dewp", y = "Humidity")

weatherDT
```
 - From the plot above, I can conclude that `dewp` and `humid` increases in similar ratio.
 
 
```{r}

weatherDT %>%
   filter(month == 7) %>%
   filter(!is.na(precip)) %>%
   ggplot(aes(x = visib, y = precip)) +
   geom_point() +
   labs(x = "Visibility", y = "Precipitation")


```
  - From the plot above, I can conclude that when the `percip` is low `visib` is higher most of the time but there are some senario where even though `precip` is high, `visib` does not change.


**4.** **Regular Expressions:** Write a regular expression to match patterns in the following strings. Demonstrate that your regular expression indeed matched that pattern by including codes and results. Carefully review how the first problem is solved for you.  

```{r}

library(stringr)


```


   a) We have a vector `vText` as follows. Write a regular expression that matches `g, og, go or ogo` in `vText` and replace the matches with '.'.
```{r}
vText <- c('google','logo','dig', 'blog', 'boogie' )
```

**Answer:**
```{r}
pattern <- 'o?go?'
gsub(pattern, '.', vText)
```

   b) Replace only the 5 or 6 digit numbers with the word "found" in the following vector. Please make sure that 3, 4, or 7 digit numbers do not get changed.
```{r}
vPhone <- c('874','6783','345345', '32120', '468349', '8149674' )




gsub('^\\d{5,6}$','found',vPhone)


```  

   c) Replace all the characters that are not among the 26 English characters or a space. Please replace with an empty spring.
```{r}
myText <- "#y%o$u @g!o*t t9h(e) so#lu!tio$n c%or_r+e%ct"



gsub("[^A-Za-z///' ]","",myText)


```  

  d) In the following text, replace all the words that are exactly 3 or 4 characters long with triple dots `...'
```{r}
myText <- "Each of the three and four character words will be gone now"



myText <- unlist(strsplit(myText, split=" "))
myText <- gsub('^\\w{3,4}$','...',myText)
paste(myText, collapse = " ")
```  

   e) Extract all the three numbers embedded in the following text.
```{r}
bigText <- 'There are four 20@14 numbers hid989den in the 500 texts'



pattern <- "\\d+"
str_extract_all(bigText,pattern)

```  

   f) Extract all the words between parenthesis from the following string text and count number of words.
```{r}
myText <- 'The salries are reported (in millions) for every company.'
myText



myText <- unlist(str_extract_all(myText,'(?<=\\().+?(?=\\))'))

myText

num_words <- sapply(strsplit(myText, split = " "), length)

num_words
```  

   g) Extract the texts in between _ and dot(.) in the following vector. Your output should be 'bill', 'pay', 'fine-book'.  
```{r}
myText <- c("H_bill.xls", "Big_H_pay.xls", "Use_case_fine-book.pdf")


str_extract_all(myText,'(?<=\\_)([a-zA-Z]|\\-)+?(?=\\.)')


```  

   h) Extract the numbers (return only integers) that are followed by the units 'ml' or 'lb' in the following text.   
```{r}
myText <- 'Received 10 apples with 200ml water at 8pm with 15 lb meat and 2lb salt'



str_extract_all(myText,'[0-9]+\\s?(ml|lb)')


```  

   i) Extract only the word in between pair of symbols `$`. Count number of words you have found between pairs of dollar sign `$`.  
```{r}
myText <- 'Math symbols are $written$ in $between$ dollar $signs$'





myText <- unlist(str_extract_all(myText,'\\$(\\w+)\\$'))

myText

length(myText)

```  

   j) Extract all the valid equations in the following text.
```{r}
myText <- 'equation1: 2+3=5, equation2 is: 2*3=6, do not extract 2w3=6'



str_extract_all(myText, '[0-9](\\+|\\*)[0-9]\\=[0-9]')


```  

   k) Extract all the letters of the following sentence and check if it contains all 26 letters in the alphabet. If not, produce code that will return the total number of unique letters that are included and list the letters that are not present as unique elements in a single vector.
```{r}
myText <- 'there are five wizard boxing matches to be judged'





myText <- str_extract_all(myText,'[a-z]')
myText_unique <- unique(unlist(myText))
length(myText_unique)

letters[which(!letters %in% myText_unique)]
```  
        
**5.** **Extracting data from the web:** Our plan is to extract data from web sources. This includes email addresses, phone numbers or other useful data. The function `readLines()` is very useful for this purpose.  



   a) Read all the text in http://mamajumder.github.io/index.html and store your texts in `myText`. Show first few rows of `myText` and examine the structure of the data. 
   
```{r}
myText <- readLines("http://mamajumder.github.io/index.html")
head(myText)
```   
   
   b) Write a regular expression that would extract all the http web links addresses from `myText`. Include your codes and display the results that show only the http web link addresses and nothing else.  
   
```{r}
elist <- str_extract_all(myText,'(http)\\://[_a-z0-9-]+\\.[_a-z0-9-]+\\.[_a-z0-9-]+(/[_a-z0-9-]+)')
unlist(elist)
```

   c) Now write a regular expression that would extract all the emails from `myText`. Include your codes and display the results that show only the email addresses and nothing else. 
   
```{r}
regex_pattern <- '[_a-z0-9-]+(\\.[_a-z0-9-]+)*\\@[_a-z0-9-]+\\.[_a-z0-9-]+'
unique(unlist(str_extract_all(myText, regex_pattern)))
```


   
   d) Now we want to extract all the phone/fax numbers in `myText`. Write a regular expression that would do this. Demonstrate your codes showing the results.  
   
```{r}
regex_pattern <- '\\(?\\d{3}\\)?(\\.| |-)?\\d{3}(\\.| |-)?\\d{4}'
unlist(str_extract_all(myText, regex_pattern))
```
   
   e) The link of ggplot2 documentation is http://docs.ggplot2.org/current/ and we would like to get the list of ggplot2 geoms from there. Write a regular expression that would extract all the geoms names (geom_bar is one of them) from this link and display the unique geoms. How many unique geoms does it have?  


```{r}
ggplot2_link <- readLines('http://docs.ggplot2.org/current/')
gg_regex_pattern <- '(geom)\\_[_a-z0-9-]+'
unique(unlist(str_extract_all(ggplot2_link, gg_regex_pattern)))
```



**6.** **Big data problem:** Download the sample of big data from canvas. Note that the data is in csv format and compressed for easy handling. Now answer the following questions.  


```{r}

library(reshape2)


```


   a) \label{select-few} Read the data and select only the columns that contains the word 'human'. Store the data in an object `dat`. Report first few rows of your data.  
   
```{r}
bigdata_canvas <- read.csv('bigDataSample.csv')

dat <- bigdata_canvas %>%
   select(contains('human'))

head(dat)
```


   b) The data frame `dat` should have 5 columns. Rename the column names keeping only the last character of the column names. So each column name will have only one character. Report first few rows of your data now.  
   
   
```{r}
dat_column_char_count <- nchar(colnames(dat))
dat_column_char_count

dat <- dat %>%
  rename_all(
     funs(
        substr(colnames(dat),dat_column_char_count, dat_column_char_count + 1)
     )
  )
head(dat)
```


   c) Compute and report the means of each columns group by column b in a nice table.  

```{r}
dat %>%
   group_by(b) %>%
   summarise(g=mean(g), p=mean(p), e=mean(e), n=mean(n))
```   
   
   d) Change the data into long form using id='b' and store the data in `mdat`. Report first few rows of data.  
   
```{r}

mdat <- melt(dat, id.vars = 'b')

head(mdat)

```

   e) The data frame `mdat` is now ready for plotting. Generate density plots of value, color and fill by variable and facet by b.  
   
```{r}

   ggplot(mdat)+
   geom_density(aes(value, fill = variable))+
   facet_wrap(~b)

```   

   f) The data set `bigDataSample.csv` is a sample of much bigger data set. Here we read the data set and then selected the desired column. Do you think it would be wise do the same thing with the actual larger data set? Explain how you will solve this problem of selecting few columns (as we did in question 6a) without reading the whole data set first. Demonstrate that showing your codes.  

 - It would never be wise to import everything because, first of all we will not read them manually and secondly we will only need the column names to do analysis.
 
 We can use the simple R code to solve this problem
 
```{r}
library(dplyr)


#data_table %>%
  #select(A, B, E)
 
```



**7.** **Optional bonus question (5 points extra)** Download the excel file "clean-dat-before.xls" from canvas It contains time series data for many variables. Among the two columns of the data, the first column represents time and the second column represents the measurement. The challange is that variable names are also inluded in the time column. Our goal is to clean and reshape the data. First few rows and columns of the desired output is shown below. Notice each time point is converted into an integer time index to make a uniform elapsed time for all the variables.

| elapse_time|   Area| Bulk.Rotation.|     ECG| Endo.MA.Circ..Strain| Endo.MA.Radial.Strain|
|-----------:|------:|--------------:|-------:|--------------------:|---------------------:|
|           1| 10.924|          0.000| 0.32157|                0.000|                 0.000|
|           2| 10.648|          0.070| 0.58824|               -1.495|                 0.762|
|           3| 10.574|         -0.128| 0.81176|               -1.423|                 2.619|
|           4| 10.487|          0.097| 0.88627|               -0.620|                 3.591|
|           5| 10.342|          0.181| 0.87451|               -1.142|                 3.472|
|           6|  9.995|          0.235| 0.85882|               -3.269|                 5.812|


 - I followed the following steps:
 - 1: First of all, I cleaned the data by removing the unwanted values, such as "File Name: IM_0004 (DICOM Native 2D [iE33])"
 - 2: I then put the "Area" column next to the actual value.. eg: :Area ECG"
 - 3: Now each of them are only 1 row apart
 - 4: after that I used the code below


```{r}


library(readxl)
library(dplyr)
library(tidyr)

excel_DF <- readxl::read_excel("clean-dat-before.xlsx", col_names = FALSE)

df <- excel_DF %>%
  drop_na() %>%
  split(., 0:(nrow(.)-1) %/% 186) %>%
  bind_cols()

df <- df[!duplicated(as.list(df))]

colnames(df) <- df[1, ]
df <- df[-1, ]
df[ , -1] <- sapply(df[ , -1], as.numeric)
df

```


